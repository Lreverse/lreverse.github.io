---
title: 深度学习
date: 2023-11-09 20:54:13 +0800
categories: [Course, 人工智能导论]
math: true
mermaid: true
---


## 1. **前馈神经网络**

### 感知机

- 激活函数：对输入信息进行非线性变换
	- 常用的激活函数：$softmax$函数
	- $sigmoid$：$f(x)=\frac{1}{1+e^{-x}}$
	- $tanh$：$f(x)=\frac{1-e^{-2x}}{1+e^{-2x}}$
	- $ReLU$：$f(x)=0, for~x\leq 0; x, for~x>0$
- 损失函数：用来计算模型预测值与真实值之间的误差
	- 常用的损失函数
		- 均方误差损失函数
			- $MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i-\widehat{y_i})^2$
		- 交叉熵损失函数
			- 主要用来度量两个概率分布间的差异
			- 假定p和q是数据x的两个概率分布，通过q来表示p的交叉熵如下：
			- $H(p,q)=-\sum_xP(x)*log~q(x)$

### 多层感知机

- 由输入层、输出层和至少一层的隐藏层构成
	- 网络中各个隐藏层中神经元可接收相邻前序隐藏层中所有神经元传递而来的信息，经过加工处理后将信息输出给相邻后续隐藏层中所有神经元

#### 参数优化

- **梯度下降**
	- 是一种使得损失函数最小化的方法
- **误差反向传播（BP）**
	- 是一种将输出层误差反向传播给隐藏层进行参数更新的方法
	- 将误差从后向前传递、将误差分摊给各层所有单元，从而获得各层单元所产生的误差，进而依据这个误差来让各层单元负起各自责任、修正各单元参数


## 2. **卷积神经网络（CNN）**

- **卷积层**
	- 卷积核：二维矩阵，矩阵中数值为权重，用来计算图像中与卷积核同样大小的子块像素点的卷积
	- 填充层
		- 扩张输入层的大小，以便卷积能得到更大的输出层
	- $n_0=\frac{n_1+2p-f}{s}+1$
		- 其中，$n_1$为输入图像的大小，$f$代表滤波器（卷积核）的大小，$s$代表步长，$p$代表填充层数，$n_0$代表输出图像的大小
- **池化层**
	- 夹在连续的卷积层中间，用于压缩数据和参数的量，减小过拟合
	- 由于图像中存在较多冗余，在图像处理中，可用某一区域子块的统计信息（如最大值或均值等）来刻画该区域中所有像素点呈现的空间分布模式，以替代区域子块中所有像素点取值
- **全连接层（FC）**
	- 每层的每一个神经元都连着近邻层的所有神经元，以这种连接关系的层就叫做全连接层


## 3. **循环神经网络（RNN）**

- 在学习过程中记住部分已经出现的信息，并利用所记住的信息影响后续结点输出

> 待完善...
{: .prompt-info }
