---
title: 机器学习
date: 2023-10-16 20:12:40 +0800
categories: [Course, 人工智能导论]
math: true
mermaid: true
---


> “数据驱动学习”的范式，从数据出发来学习数据中的所蕴含的模式，对数据进行抽象
{: .prompt-info }

concept：如果程序在解决任务T上的表现P，可以通过经验E得到改进，则说程序被认为能够从经验E中学习
- 分类
	- 半监督学习
		- **监督学习**
			- 数据有标签、一般为回归或分类等任务
		- **无监督学习**
			- 数据无标签、一般为聚类或若干降维任务
	- **强化学习**
		- 序列数据决策学习，一般为从与环境交互中学习


## 1. **监督学习**

- 没有免费午餐定理（NFL）
	- 任何机器学习模型在所有问题上的性能都是相同的，其总误差和模型本身是没有关系的。一种算法（算法A）在特定数据集上的表现优于另一种算法（算法B）的同时，一定伴随着算法A在另外某一个特定的数据集上有着不如算法B的表现
- 重要元素
	- 标注数据、学习模型、损失函数
- **损失函数**
	- 对于训练集中的数据$(x_i, y_i)$，$y_i$是$x_i$的标注信息，$f$为对$x$的预测结果
	- 损失函数为计算$x_i$的真实值$y_i$与预测值$f(x_i)$之间差值的函数$Loss(y_i, f(x_i))$
- 经验风险
	- 训练集中数据产生的损失
	- 经验风险越小，说明学习模型对训练数据拟合程度越好
	- 是模型关于训练样本集平均损失$\frac{1}{n}\sum_{i=1}^{n}Loss(y_i,f(x_i))$
- 期望风险
	- 测试集中存在无穷多数据时产生的损失
	- 期望风险越小，学习所得模型越好
	- 是模型关于联合分布期望损失$\int_{x\times y}Loss(y,f(x))P(x,y)dxdy$
- 核心思想
	- 从原始数据中提取特征
	- 学习映射函数f
	- 通过映射函数f将原始数据映射到语义任务空间（即寻找数据和任务目标之间的关系）

#### 分类
- **判别方法**
	- 直接学习判别函数$f(x)$或者条件概率分布$P(Y \mid X)$作为预测的模型，即**判别模型**
		- 判别模型关系在给定的输入数据下，预测该数据的输出是什么
		- 典型判别模型包括回归模型、神经网络、支持向量机和Ada boosting等
- **生成方法**
	- **生成模型**从数据中学习联合概率分布$P(X, Y)$
		- 典型方法：贝叶斯方法、隐马尔可夫链

### 回归分析

> 分析不同变量之间存在关系的研究

##### 线性回归（linear regression）

- **一元线性回归**
	- 回归模型：$y = ax + b$
	- 最佳回归模型：使误差总和$L(a,b)$值最小
		- 使用最小二乘法（对上式求偏导）
		- $a=\frac{\sum_{i=1}^{n}x_iy_i-n\overline x \overline y}{\sum_{i=1}^{n}x_i^2-n\overline x^2}$
		- $b=\overline y -a\overline x$
- **多元线性回归**

### K近邻算法

> 简称**KNN算法**

- 思路：如果一个样本在特征空间中的k个最相似的样本（即特征空间中最邻近）中的大多数属于某一个类别，则该样本也属于这个类别
	- 所选择的邻居都是已经正确分类的对象
	- 该方法在分类决策上只依据最邻近的一个或几个样本的类别来决定待分类样本所属的类别
- 算法过程
	- 输入：训练数据集$T=\{(x_i, y_i)\}$
	- 输出：实例x所属的类别y
		- 根据给定的距离度量，在训练集T中找出与x最近的k个点，涵盖着k个点的x的邻域记作$N_k(x)$
		- 在$N_k(x)$中根据分类决策规则决定x所属的类别y

### 决策树

- 一种通过树形结构来进行分类的方法
	- 树形结构中每个非叶子节点表示对分类目标在某个属性上的一个判断，每个分支代表基于该属性做出的一个判断
	- 最后树形结构中每个叶子节点代表一种分类结果，所以决策树可以看作是一系列以叶子节点为输出的决策规则

### 生成学习模型—朴素贝叶斯方法

#### 数字识别模型

- 输入：图像/像素网格
- 输出：一个数字0-9
	- 数据：获取一个大量的示例图像集合，每个图像都标有标注（人手动标记）
	- 特征：用于进行数字决策的属性
- 关键点：需要学习每个节点的概率分布
	- 先验概率：训练集中每个数字出现的频率$P(y)$
	- 条件概率：对于每个数字，需要计算每个像素点在该数字出现时的条件概率$P(F_i \mid y)$
- 通过**平滑或正则化**可用减少过拟合的风险，提高模型在新数据上的泛化能力


## 2. **无监督学习**

> **不需要标签**，而是通过对数据的分析和处理来发现数据中的规律、关系和内在结构
{: .prompt-info }

- 主要任务
	- **聚类**：将数据分成不同的组和簇，使得同一组内的数据相似度较高，不同组之间的数据相似度较低
	- **降维**：将高维数据转换为低维数据，以便更好地可视化和理解数据
	- **关联规则挖掘**：发现数据中的频繁项集和关联规则，以便了解数据中的关联性和依赖性

#### K均值聚类

输入：n个数据
输出：k个聚类结果
目的：将n个数据聚类到k个集合
- 步骤
	1. 初始化聚类质心
	2. 将每个待聚类数据放入唯一一个聚类集合中
		- 计算待聚类数据$x_i$和质心$c_j$之间的欧式距离$d(x_i, c_j)$
		- 将每个$x_i$放入与之距离最近聚类质心所在聚类集合中，即$argmin~d(x_i, c_j)$
	3. 依据聚类结果，更新聚类质心
		- 根据每个聚类集合中所包含的数据，更新该聚类集合质心值
	4. 算法循环迭代，直到满足条件
	5. 聚类迭代满足如下任意一个条件，则聚类停止
		- 已经达到了迭代次数上限
		- 前后两次迭代中，聚类质心基本保持不变


## 3. 强化学习

马尔可夫决策过程

> 待完善...
{: .prompt-info }
